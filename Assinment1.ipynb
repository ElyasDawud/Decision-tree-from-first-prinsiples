{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "bknote = pd.read_csv('data_banknote_authentication.txt',header = None).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data,test_size):\n",
    "    \"\"\" function that takes data (2d-array) and \n",
    "        test size (percentage)\n",
    "        and returns the two data sets train and test\n",
    "        test rows are selected randomly\n",
    "    \"\"\"\n",
    "    nr_rows = data.shape[0]\n",
    "    np.random.seed(3)\n",
    "    test_rows = np.random.choice(nr_rows, size=round((test_size*nr_rows)), replace=False)\n",
    "    test_ar = data[test_rows, :]\n",
    "    train_ar = np.delete(data, test_rows, axis=0)\n",
    "    return train_ar,test_ar\n",
    "#train_test_split(bknote,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sameLabel(data):\n",
    "    \"\"\"This function checks if all train points belong in the same class\n",
    "        argument: data, 2d-array\n",
    "        return : boolean \n",
    "    \"\"\"\n",
    "    all_classes = np.unique(data[:,-1])\n",
    "    if(len(all_classes)>=2):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate(data):\n",
    "    \"\"\" For the given data, this function returns \n",
    "        the most commonly occuring class\n",
    "    \"\"\"\n",
    "    labels,occurrence = np.unique(data[:,-1], return_counts=True)\n",
    "    return labels[occurrence.argmax()]\n",
    "\n",
    "#allocate(bknote[700:,])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_points(data):\n",
    "    \"\"\" limit potential split points by considering only cases where \n",
    "        class lavel changes between to adjacent points\n",
    "    \"\"\"\n",
    "    split_points = {}\n",
    "    nr_columns = data.shape[1]\n",
    "    for i in range(nr_columns-1):\n",
    "        split_points[i] = []\n",
    "        unique_points = np.unique(data[:,i])\n",
    "        for k in range(1,len(unique_points)):\n",
    "            potential_points = (unique_points[k]+unique_points[k-1])/2\n",
    "            split_points[i].append(potential_points)\n",
    "    return split_points\n",
    "#split_points(bknote)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(data,split_attribute,split_point):\n",
    "    \"\"\" This function separates the data by an attribute at \n",
    "        a given split point, returns two 2d arrays, one left\n",
    "        to the split point and one to the right\n",
    "    \"\"\"\n",
    "    column = data[:,split_attribute]\n",
    "    bool_ar_left = column <= split_point\n",
    "    bool_ar_right = column > split_point\n",
    "    left = data[bool_ar_left,]\n",
    "    right = data[bool_ar_right,]\n",
    "    return left, right\n",
    "#separate_data(bknote,2,bknote[:,0].mean())[0]\n",
    "#separate_data(bknote,0,bknote[:,0].mean())[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data):\n",
    "    #calculate entropy: information content\n",
    "    label_counts = np.unique(data[:,-1],return_counts = True)[1]\n",
    "    proportions = label_counts/label_counts.sum()\n",
    "    entropy = np.sum(-proportions*np.log2(proportions))\n",
    "    return entropy\n",
    "\n",
    "#entropy(bknote) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(data):\n",
    "    #calculate impurity\n",
    "    label_counts = np.unique(data[:,-1],return_counts = True)[1]\n",
    "    proportions = label_counts/label_counts.sum()\n",
    "    gini = 1-np.sum(proportions**2)\n",
    "    return gini\n",
    "#gini_index(bknote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_gain(data,left,right, impurity_measure):\n",
    "    if impurity_measure == 'entropy':\n",
    "        # calculate the information at a given node\n",
    "        entropy_before = entropy(data)\n",
    "        prop_left = left.shape[0]/data.shape[0]\n",
    "        prop_right = right.shape[0]/data.shape[0]\n",
    "        entropy_after = prop_left*entropy(left) + prop_right*entropy(right)\n",
    "        inf_gain = entropy_before - entropy_after\n",
    "        return inf_gain\n",
    "    elif impurity_measure == 'gini':\n",
    "        gini_before = gini_index(data)\n",
    "        prop_left = left.shape[0]/data.shape[0]\n",
    "        prop_right = right.shape[0]/data.shape[0]\n",
    "        gini_after = prop_left*gini_index(left) + prop_right*gini_index(right)\n",
    "        inf_gain = gini_before - gini_after\n",
    "        return inf_gain\n",
    "        \n",
    "#inf_gain(bknote,separate_data(bknote,0,bknote[:,0].mean())[0],separate_data(bknote,0,bknote[:,0].mean())[1],'gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_possible_split(data,impurity_measure):\n",
    "    \"\"\" This function, given data, determines the best split \n",
    "        by iterating over possible splits, and returns the split\n",
    "        attribute and split point\n",
    "    \"\"\"\n",
    "    max_infgain = -1 # since information gain ranges between 0 and 1\n",
    "    possible_splits = split_points(data)\n",
    "    for c_index in possible_splits:\n",
    "        for value in possible_splits[c_index]:\n",
    "            left,right = separate_data(data,c_index,value)\n",
    "            infgain = inf_gain(data,left,right,impurity_measure)\n",
    "            \n",
    "            if infgain > max_infgain:\n",
    "                max_infgain = infgain\n",
    "                split_col = c_index\n",
    "                sp_point = value\n",
    "    return split_col,sp_point\n",
    "#best_possible_split(bknote,'gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_pre_pruning(data,impurity_measure = 'entropy',pruning=False):\n",
    "    \"\"\" This reccursive function  learns a decision tree classifier from a data matrix X\n",
    "        and a label vector y.\n",
    "        Both X and y are needed to be numppy arrays\n",
    "    \"\"\"\n",
    "    # since all the helper fucntions take an entire array\n",
    "    #y = np.transpose([list(y)])\n",
    "    #data = np.append(X, y, axis=1) \n",
    "    # simplest case\n",
    "    if sameLabel(data):\n",
    "        classification = allocate(data)\n",
    "        return classification\n",
    "    else:\n",
    "        possible_splits = split_points(data)\n",
    "        best_split = best_possible_split(data,impurity_measure)\n",
    "        left,right = separate_data(data,best_split[0],best_split[1])\n",
    "        \n",
    "        split_cond = f\"{best_split[0]} <= {best_split[1]}\"\n",
    "        sub_tree = {split_cond:[]}\n",
    "        cond_true = learn(left,impurity_measure)\n",
    "        cond_false = learn(right,impurity_measure)\n",
    "\n",
    "        sub_tree[split_cond].append(cond_true)\n",
    "        sub_tree[split_cond].append(cond_false)        \n",
    "        \n",
    "        return sub_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(row,tree):\n",
    "    \"\"\" This function classifies data  the class they belong given a fitted tree\n",
    "        Argument: a row\n",
    "        return: predicted label for a given row\n",
    "    \"\"\"\n",
    "    split_condition = list(tree.keys())[0]\n",
    "    split_attribute = int(split_condition.split()[0])\n",
    "    sp_point = float(split_condition.split()[2])\n",
    "    \n",
    "    if row[split_attribute] <= sp_point:\n",
    "        answer = tree[split_condition][0]\n",
    "    else:\n",
    "        answer = tree[split_condition][1]\n",
    "        \n",
    "    if not isinstance(answer,dict):\n",
    "        return answer\n",
    "    else:\n",
    "        rest_of_tree = answer\n",
    "        return classifier(row,rest_of_tree)\n",
    "    \n",
    "#classifier(bknote[900,:],tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data,tree):\n",
    "    \"\"\" Main function to predict classes after fitting a model,\"\"\"\n",
    "    pred = list()\n",
    "    for i in range(data.shape[0]):\n",
    "        pred.append(classifier(data[i,:],tree))\n",
    "    return pred\n",
    "#predict(bknote[0:5],tree)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_pruning(tree,train,val):\n",
    "    \"\"\" helper function to the pruned function\"\"\"\n",
    "    labels,counts = list(np.unique(train[:,-1],return_counts = True))\n",
    "    leaf = labels[list(counts).index(max(counts))]\n",
    "    # now check which makes the most errors\n",
    "    errors_leaf = sum(val[:,-1] != leaf)\n",
    "    errors_at_node = sum(val[:,-1] != predict(val,tree))\n",
    "    if errors_leaf <= errors_at_node:\n",
    "        return leaf\n",
    "    else:\n",
    "        return tree\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruned(tree,train,val):\n",
    "    \"\"\" Function that prunes a given tree, given training \n",
    "        and validation sets. predicts a leaf node for each \n",
    "        tree node that perforsm less than or equal to the \n",
    "        majority class, returns a pruned tree\n",
    "    \"\"\"\n",
    "    split_condition = list(tree.keys())[0]\n",
    "    yes, no = tree[split_condition]\n",
    "    # stopping \n",
    "    if not isinstance(yes,dict) and not isinstance(no,dict):\n",
    "        return after_pruning(tree,train,val)  \n",
    "    else:\n",
    "        split_attribute = int(split_condition.split()[0])\n",
    "        sp_point = float(split_condition.split()[2])\n",
    "        train_left,train_right = separate_data(train,split_attribute,sp_point)\n",
    "        val_left, val_right = separate_data(val,split_attribute,sp_point)\n",
    "        \n",
    "        if isinstance(yes,dict):\n",
    "            tree[split_condition][0] = pruned(yes,train_left,val_left)\n",
    "        if isinstance(no,dict):\n",
    "            tree[split_condition][1] = pruned(no,train_right,val_right)\n",
    "            \n",
    "        return after_pruning(tree,train,val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(data,impurity_measure,pruning = False,val_size = 0.15):\n",
    "    tree = learn_pre_pruning(data,impurity_measure)\n",
    "    if pruning == True:\n",
    "        train,validate = train_test_split(data,val_size)\n",
    "        return pruned(tree,train,validate)\n",
    "    else:\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(labels,predicted_labels):\n",
    "    \"\"\" Calculate prediction accuracy given predicted and true labesl\"\"\"\n",
    "    #calculate prediction accuracy\n",
    "    mis_classified = 0\n",
    "    if not len(labels) == len(predicted_labels):\n",
    "        return 'true values and predicted ones differ in size'\n",
    "    else:\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] != predicted_labels[i]:\n",
    "                mis_classified +=1\n",
    "    return (mis_classified/len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that the learn function is ready, use it to train a classifier on the banknote dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#start by splitting the data into training,validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(bknote,0.15)\n",
    "training_set, validation_set = train_test_split(train,0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the different trees \n",
    "tree_unpruned_gini = learn(training_set,'gini')\n",
    "tree_unpruned_entropy = learn(training_set,'entropy')\n",
    "tree_pruned_gini = learn(training_set,'gini',pruning = True)\n",
    "tree_pruned_entropy = learn(training_set,'entropy',pruning = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy  for the unpruned tree by gini is: 0.9771428571428571\n",
      "The validation accuracy  for the pruned tree by gini is:  0.9428571428571428\n",
      "The validation accuracy  for the unpruned tree by entropy is: 0.9828571428571429\n",
      "The validation accuracy  for the pruned tree by entropy is:  0.9771428571428571\n"
     ]
    }
   ],
   "source": [
    "trees = {'gini':[tree_unpruned_gini,tree_pruned_gini],'entropy':[tree_unpruned_entropy,tree_pruned_entropy]}\n",
    "for key in trees.keys():\n",
    "    print(f\"The validation accuracy  for the unpruned tree by {key} is: {1-calc_accuracy(predict(validation_set,trees[key][0]),validation_set[:,-1])}\")\n",
    "    print(f\"The validation accuracy  for the pruned tree by {key} is:  {1-calc_accuracy(predict(validation_set,trees[key][1]),validation_set[:,-1])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy  for the chosen tree is: 0.9805825242718447\n"
     ]
    }
   ],
   "source": [
    "# the unpruned tree where entropy is the the impurity measure \n",
    "# performs best on the validation set\n",
    "print(f\"The test accuracy  for the chosen tree is: {1-calc_accuracy(predict(test,tree_unpruned_entropy),test[:,-1])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparison with existing implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(training_set[:,0:4],training_set[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5 ms ± 29.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit clf.fit(training_set[:,0:4],training_set[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The test accuracy for sklearn clf is 0.9902912621359223'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"The test accuracy for sklearn clf is {1-calc_accuracy(test[:,-1],clf.predict(test[:,0:4]))}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.09 s ± 47.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# we can see that is slight more accurate that the tree I've trained\n",
    "# let us see speed differences \n",
    "%timeit learn(training_set,'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243 ns ± 4.2 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
